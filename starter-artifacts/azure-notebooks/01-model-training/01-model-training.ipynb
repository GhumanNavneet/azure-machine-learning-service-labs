{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 1 - load the data #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport pandas as pd\nimport os\n\nprint(\"Current working directory is \", os.path.abspath(os.path.curdir))\ndf = pd.read_csv('./data/UsedCars_Clean.csv', delimiter=',')\nprint(df.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 2 - add the affordable feature #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "df['Affordable'] = np.where(df['Price'] < 12000, 1, 0)\ndf_affordability = df[[\"Age\", \"KM\", \"Affordable\"]]\nprint(df_affordability.head())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 3 - Scale the numeric feature values #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "X = df_affordability[[\"Age\", \"KM\"]].values\ny = df_affordability[[\"Affordable\"]].values\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nprint(pd.DataFrame(X).describe().round(2))\nprint(pd.DataFrame(X_scaled).describe().round(2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 4 - Fit a Logistic Regression #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn import linear_model\n\n# Create a linear model for Logistic Regression\nclf = linear_model.LogisticRegression(C=1)\n\n# we create an instance of Classifier and fit the data.\nclf.fit(X_scaled, y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 5 - Test the trained model's prediction #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "age = 60\nkm = 40000\n\nscaled_input = scaler.transform([[age, km]])\nprediction = clf.predict(scaled_input)\n\nprint(\"Can I afford a car that is {} month(s) old with {} KM's on it?\".format(age, km))\nprint(\"Yes (1)\" if prediction[0] == 1 else \"No (0)\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 6 - Measure the model's performance #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "scaled_inputs = scaler.transform(X)\npredictions = clf.predict(scaled_inputs)\nprint(predictions)\n\nfrom sklearn.metrics import accuracy_score\n\nscore = accuracy_score(y, predictions)\nprint(\"Model Accuracy: {}\".format(score.round(3)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 7 - Define a method to experiment with different training subset sizes #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nfull_X = df_affordability[[\"Age\", \"KM\"]]\nfull_Y = df_affordability[[\"Affordable\"]]\n\ndef train_eval_model(full_X, full_Y, training_set_percentage):\n    train_X, test_X, train_Y, test_Y = train_test_split(full_X, full_Y, train_size=training_set_percentage,\n                                                        random_state=42)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(train_X)\n    clf = linear_model.LogisticRegression(C=1)\n    clf.fit(X_scaled, train_Y)\n\n    scaled_inputs = scaler.transform(test_X)\n    predictions = clf.predict(scaled_inputs)\n    score = accuracy_score(test_Y, predictions)\n\n    return (clf, score)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 8 - Verify AML SDK Installed #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import azureml.core\n\nprint(\"SDK Version:\", azureml.core.VERSION)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 9 - Create a workspace #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Provide the Subscription ID of your existing Azure subscription\nsubscription_id = \"xxx-xxx-xxx\"\n\n# Provide values for the Resource Group and Workspace that will be created\nresource_group = \"aml-workspace-z\"\nworkspace_name = \"aml-workspace-z\"\nworkspace_region = 'westcentralus'  # eastus, westcentralus, southeastasia, australiaeast, westeurope\n\nimport azureml.core\n\n# import the Workspace class\nfrom azureml.core import Workspace\n\nws = Workspace.create(\n    name=workspace_name,\n    subscription_id=subscription_id,\n    resource_group=resource_group,\n    location=workspace_region,\n    exist_ok=True)\n\nprint(\"Workspace Provisioning complete.\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 10 - Create an experiment and log metrics for multiple training runs #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core.run import Run\nfrom azureml.core.experiment import Experiment\n\n# start a training run by defining an experiment\nmyexperiment = Experiment(ws, \"UsedCars_Experiment\")\nroot_run = myexperiment.start_logging()\n\ntraining_set_percentage = 0.25\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\ntraining_set_percentage = 0.5\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\ntraining_set_percentage = 0.75\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\ntraining_set_percentage = 0.9\nrun = root_run.child_run(\"Training_Set_Percentage-%0.5F\" % training_set_percentage)\nmodel, score = train_eval_model(full_X, full_Y, training_set_percentage)\nprint(\"With %0.2f percent of data, model accuracy reached %0.4f.\" % (training_set_percentage, score))\nrun.log(\"Training_Set_Percentage\", training_set_percentage)\nrun.log(\"Accuracy\", score)\nrun.complete()\n\n# Close out the experiment\nroot_run.complete()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 11 - Review captured runs #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Go to the Azure Portal, find your Azure Machine Learning Workspace, \n#select Experiments and select the UsedCars_Experiment\n\n# You can also query the run history using the SDK.\n# The following command lists all of the runs for the experiment\nruns = [r for r in root_run.get_children()]\nprint(runs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 12 - Create an AML Compute #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ws = Workspace.get(name=workspace_name, subscription_id=subscription_id,resource_group=resource_group)\nprint(ws.name, ws.location, ws.resource_group, ws.location, sep = '\\t')\n\nexperiment_name = \"UsedCars_ManagedCompute_01\"\n\nfrom azureml.core import Experiment\nexp = Experiment(workspace=ws, name=experiment_name)\n\nfrom azureml.core.compute import AmlCompute\nfrom azureml.core.compute import ComputeTarget\nimport os\n\n# choose a name for your cluster\nbatchai_cluster_name = \"UsedCars-02\"\ncluster_min_nodes = 1\ncluster_max_nodes = 3\nvm_size = \"STANDARD_DS11_V2\"\n\n\nif batchai_cluster_name in ws.compute_targets:\n    compute_target = ws.compute_targets[batchai_cluster_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('Found existing compute target, using this compute target instead of creating:  ' + \n              batchai_cluster_name)\n    else:\n        print(\"Error: A compute target with name \", batchai_cluster_name,\n              \" was found, but it is not of type AmlCompute.\")\nelse:\n    print('Creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, # NC6 is GPU-enabled\n                                                                min_nodes = cluster_min_nodes, \n                                                                max_nodes = cluster_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(ws, batchai_cluster_name, provisioning_config)\n    \n    # can poll for a minimum number of nodes and for a specific timeout. \n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n    \n     # For a more detailed view of current BatchAI cluster status, use the 'status' property    \n    print(compute_target.status.serialize())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 13 - Upload the dataset to the DataStore #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "ds = ws.get_default_datastore()\nprint(ds.datastore_type, ds.account_name, ds.container_name)\nds.upload(src_dir='./data', target_path='used_cars', overwrite=True, show_progress=True)\n\n# Prepare batch training script\n# - See ./training/train.py\n################################",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 14 - Create estimator #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.train.estimator import Estimator\n\nscript_params = {\n    '--data-folder': ds.as_mount(),\n    '--training-set-percentage': 0.3\n}\n\nest_config = Estimator(source_directory='./training',\n                       script_params=script_params,\n                       compute_target=compute_target,\n                       entry_script='train.py',\n                       conda_packages=['scikit-learn', 'pandas'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Step 15 - Execute the estimator job #"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "run = exp.submit(config=est_config)\nrun\n\n# Poll for job status\nrun.wait_for_completion(show_output=True)  # value of True will display a verbose, streaming log\n\n# Examine the recorded metrics from the run\nprint(run.get_metrics())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}